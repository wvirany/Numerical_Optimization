{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bfgs(f, x0, grad_f, tol=1e-6, maxiter=100):\n",
    "    n = x0.shape[0]\n",
    "    I = np.eye(n)\n",
    "    x = x0\n",
    "    H = I\n",
    "    grad = grad_f(x)\n",
    "    iter = 0\n",
    "    while np.linalg.norm(grad) > tol and iter < maxiter:\n",
    "        p = -np.dot(H, grad)\n",
    "        alpha = 1.0\n",
    "        while f(x + alpha*p) > f(x) + alpha*0.1*np.dot(grad, p):\n",
    "            alpha *= 0.5\n",
    "        s = alpha*p\n",
    "        x_new = x + s\n",
    "        grad_new = grad_f(x_new)\n",
    "        y = grad_new - grad\n",
    "        rho = 1.0 / np.dot(y, s)\n",
    "        H = (I - rho*np.outer(s, y)) @ H @ (I - rho*np.outer(y, s)) + rho*np.outer(s, s)\n",
    "        x = x_new\n",
    "        grad = grad_new\n",
    "        iter += 1\n",
    "    return [x,iter]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import line_search\n",
    "\n",
    "def bfgs2(f, grad_f, x0, tol=1e-6, max_iter=1000):\n",
    "    \n",
    "    \n",
    "    # Initialize the inverse Hessian approximation.\n",
    "    Bk = np.eye(x0.shape[0])\n",
    "\n",
    "    # Initialize the current iterate and gradient.\n",
    "    xk = x0\n",
    "    grad_f_k = grad_f(xk)\n",
    "\n",
    "    # Initialize the iteration counter.\n",
    "    k = 0\n",
    "\n",
    "    # Loop until convergence or maximum number of iterations is reached.\n",
    "    while np.linalg.norm(grad_f_k) > tol and k < max_iter:\n",
    "\n",
    "        # Compute the search direction.\n",
    "        pk = -Bk @ grad_f_k\n",
    "\n",
    "        # Perform a line search to find the step size alpha that satisfies the Wolfe conditions.\n",
    "        alpha, _, _, _, _, _ = line_search(f, grad_f, xk, pk)\n",
    "\n",
    "        # Compute the new iterate.\n",
    "        xk1 = xk + alpha * pk\n",
    "\n",
    "        # Compute the new gradient.\n",
    "        grad_f_k1 = grad_f(xk1)\n",
    "\n",
    "        # Compute the change in gradient and iterate.\n",
    "        delta_grad = grad_f_k1 - grad_f_k\n",
    "        delta_x = xk1 - xk\n",
    "\n",
    "        # Update the inverse Hessian approximation using the BFGS formula.\n",
    "        Bk = Bk + (np.outer(delta_grad, delta_grad) / (delta_grad.T @ delta_x)) \\\n",
    "             - (np.outer(Bk @ delta_x, Bk @ delta_x.T) / (delta_x.T @ Bk @ delta_x))\n",
    "\n",
    "        # Update the current iterate and gradient.\n",
    "        xk = xk1\n",
    "        grad_f_k = grad_f_k1\n",
    "\n",
    "        # Increment the iteration counter.\n",
    "        k += 1\n",
    "\n",
    "    # Return the optimal solution and function value.\n",
    "    return xk, f(xk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized solution: [0.99999997 0.99999995]\n",
      "Optimized function value: 2.5353055595473786e-15\n",
      "[array([1.        , 0.99999999]), 34]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the Rosenbrock function and its gradient\n",
    "import numpy as np\n",
    "\n",
    "def rosenbrock(x, a=1, b=100):\n",
    "    \n",
    "    return (a - x[0])**2 + b*(x[1] - x[0]**2)**2\n",
    "\n",
    "def rosenbrock_gradient(x, a=1, b=100):\n",
    "    \n",
    "    grad = np.zeros_like(x)\n",
    "    grad[0] = 2*(x[0] - a) - 4*b*x[0]*(x[1] - x[0]**2)\n",
    "    grad[1] = 2*b*(x[1] - x[0]**2)\n",
    "    return grad\n",
    "\n",
    "\n",
    "# Set the initial guess\n",
    "x0 = np.array([-1.2, 1.0])\n",
    "\n",
    "# Use the BFGS algorithm to minimize the Rosenbrock function\n",
    "result = minimize(rosenbrock, x0, method='BFGS', jac=rosenbrock_grad)\n",
    "result1 = bfgs(rosenbrock, x0, rosenbrock_grad)\n",
    "\n",
    "# Print the optimized solution and function value\n",
    "print(\"Optimized solution:\", result.x)\n",
    "print(\"Optimized function value:\", result.fun)\n",
    "print(result1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs(f, x0, grad_f, tol=1e-6, maxiter=100):\n",
    "    n = x0.shape[0]\n",
    "    I = np.eye(n)\n",
    "    x = x0\n",
    "    H = I\n",
    "    grad = grad_f(x)\n",
    "    iter = 0\n",
    "    while np.linalg.norm(grad) > tol and iter < maxiter:\n",
    "        p = -np.dot(H, grad)\n",
    "        alpha = 1.0\n",
    "        while f(x + alpha*p) > f(x) + alpha*0.1*np.dot(grad, p):\n",
    "            alpha *= 0.5\n",
    "        s = alpha*p\n",
    "        x_new = x + s\n",
    "        grad_new = grad_f(x_new)\n",
    "        y = grad_new - grad\n",
    "        rho = 1.0 / np.dot(y, s)\n",
    "        H = (I - rho*np.outer(s, y)) @ H @ (I - rho*np.outer(y, s)) + rho*np.outer(s, s)\n",
    "        x = x_new\n",
    "        grad = grad_new\n",
    "        iter += 1\n",
    "    return [x,iter]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
