{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bfgs(f, x0, grad_f, tol=1e-6, maxiter=100):\n",
    "    n = x0.shape[0]\n",
    "    I = np.eye(n)\n",
    "    x = x0\n",
    "    H = I\n",
    "    grad = grad_f(x)\n",
    "    iter = 0\n",
    "   \n",
    "    while np.linalg.norm(grad) > tol and iter < maxiter:\n",
    "        p = -np.dot(H, grad)\n",
    "        alpha = 1.0\n",
    "        while f(x + alpha*p) > f(x) + alpha*0.1*np.dot(grad, p):\n",
    "            alpha *= 0.5\n",
    "        s = alpha*p\n",
    "        x_new = x + s\n",
    "        grad_new = grad_f(x_new)\n",
    "        y = grad_new - grad\n",
    "        rho = 1.0 / np.dot(y, s)\n",
    "        \n",
    "        H = (I - rho*np.outer(s, y.T)) @ H @ (I - rho*np.outer(y, s.T)) + rho*np.outer(s, s.T)\n",
    "        x = x_new\n",
    "        \n",
    "        grad = grad_new\n",
    "        iter += 1\n",
    "    return [x,iter]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dfp(f, x0, grad_f, tol=1e-6, maxiter=100):\n",
    "    n = x0.shape[0]\n",
    "    I = np.eye(n)\n",
    "    x = x0\n",
    "    H = I\n",
    "    grad = grad_f(x)\n",
    "    iter = 0\n",
    "    while np.linalg.norm(grad) > tol and iter < maxiter:\n",
    "        p = -np.dot(H, grad)\n",
    "        alpha = 1.0\n",
    "        while f(x + alpha*p) > f(x) + alpha*0.1*np.dot(grad, p):\n",
    "            alpha *= 0.5\n",
    "        s = alpha*p\n",
    "        x_new = x + s\n",
    "        grad_new = grad_f(x_new)\n",
    "        y = grad_new - grad\n",
    "        rho = 1.0 / np.dot(y, s)\n",
    "        A = I - rho*np.outer(s, y)\n",
    "        B = I - rho*np.outer(y, s)\n",
    "        H = np.dot(A, np.dot(H, B)) + rho*np.outer(s, s)\n",
    "        x = x_new\n",
    "        grad = grad_new\n",
    "        iter += 1\n",
    "    return [x, iter]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sr1_root(f, grad_f, x0, tol=1e-6, maxiter=1000):\n",
    "    n = x0.shape[0]\n",
    "    H = np.eye(n)\n",
    "    x = x0\n",
    "    grad = grad_f(x)\n",
    "    iter = 0\n",
    "    while np.linalg.norm(grad) > tol and iter < maxiter:\n",
    "        p = -np.dot(H, grad)\n",
    "        alpha = 1.0\n",
    "        while f(x + alpha*p) > f(x) + alpha*0.1*np.dot(grad, p):\n",
    "            alpha *= 0.5\n",
    "        s = alpha*p\n",
    "        x_new = x + s\n",
    "        grad_new = grad_f(x_new)\n",
    "        y = grad_new - grad\n",
    "        # Compute the update s that results in the change in gradient y\n",
    "        if np.abs(np.dot(s, y)) >= tol*np.linalg.norm(s)*np.linalg.norm(y):\n",
    "            # SR1 update if the denominator is non-zero\n",
    "            \n",
    "            H += np.outer((s - np.dot(H, y)), (s - np.dot(H, y))) / np.dot(s - np.dot(H, y), y)\n",
    "        # If the denominator is close to zero, skip the update\n",
    "        x = x_new\n",
    "        grad = grad_new\n",
    "        iter += 1\n",
    "    return [x, iter]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized solution: [0.99999997 0.99999995]\n",
      "Optimized function value: 2.5353055595473786e-15\n",
      "[array([1.        , 0.99999999]), 34]\n",
      "[array([1.        , 0.99999999]), 34]\n",
      "[array([nan, nan]), 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-48-7779bab2f3ee>:46: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  H += np.outer((s - np.dot(H, y)), (s - np.dot(H, y))) / np.dot(s - np.dot(H, y), y)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize as opt\n",
    "\n",
    "# Define the Rosenbrock function and its gradient\n",
    "import numpy as np\n",
    "\n",
    "def rosenbrock(x, a=1, b=100):\n",
    "    \n",
    "    return (a - x[0])**2 + b*(x[1] - x[0]**2)**2\n",
    "\n",
    "def rosenbrock_gradient(x, a=1, b=100):\n",
    "   \n",
    "    grad = np.zeros_like(x)\n",
    "    grad[0] = 2*(x[0] - a) - 4*b*x[0]*(x[1] - x[0]**2)\n",
    "    grad[1] = 2*b*(x[1] - x[0]**2)\n",
    "    return grad\n",
    "\n",
    "# Set the initial guess\n",
    "x0 = np.array([-1.2, 1.0])\n",
    "x1 = np.array([0.5,0.5])\n",
    "# Use the BFGS algorithm to minimize the Rosenbrock function\n",
    "result = minimize(rosenbrock, x0, method='BFGS', jac=rosenbrock_grad)\n",
    "result1 = bfgs(rosenbrock, x0, rosenbrock_grad)\n",
    "result2 = dfp(rosenbrock, x0, rosenbrock_grad, )\n",
    "result3= sr1_root(rosenbrock, rosenbrock_grad, x0)\n",
    "# Print the optimized solution and function value\n",
    "print(\"Optimized solution:\", result.x)\n",
    "print(\"Optimized function value:\", result.fun)\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
